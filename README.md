# ğŸŒŸ HeartSense 

Bienvenue dans **HeartSense**, une application moderne, puissante et 100 % open source conÃ§ue pour rÃ©pondre Ã  vos besoins en toute simplicitÃ©. CrÃ©Ã©e avec soin, cette solution est **faite maison**, entiÃ¨rement **auto-hÃ©bergÃ©e**, et dÃ©diÃ©e aux passionnÃ©s de libertÃ© numÃ©rique et de transparence.

---

## ğŸš€ FonctionnalitÃ©s Principales

### ğŸŒ **Application auto-hÃ©bergÃ©e**
- Gardez un contrÃ´le total sur vos donnÃ©es.
- DÃ©ployez facilement votre instance sur votre serveur.
- Aucun besoin de dÃ©pendances externesâ€¯: tout est optimisÃ© pour une utilisation privÃ©e.

### ğŸ› ï¸ **EntiÃ¨rement open source**
- Code source disponible sur [GitHub](#).
- Contributions bienvenues pour amÃ©liorer et enrichir le projet.
- Auditable pour garantir une transparence maximale.

### âœ¨ **Fait maison, pour tous**
- ConÃ§u pour les petites Ã©quipes, les dÃ©veloppeurs et les passionnÃ©s d'open source.
- Interface intuitive et fonctionnalitÃ©s avancÃ©es, dÃ©veloppÃ©es avec amour.

### ğŸ”’ **SÃ©curisÃ© et Ã©volutif**
- Gestion des utilisateurs et permissions personnalisables.
- OptimisÃ© pour une performance fluide sur des serveurs lÃ©gers.

---

## ğŸ¯ Objectifs du Projet

MonProjet vise Ã  offrir une solution clÃ© en main, simple Ã  dÃ©ployer et facile Ã  adapter selon vos besoins. 
Lâ€™accent est mis surâ€¯:
- **Transparence** : Pas de surprise cachÃ©e. Vous contrÃ´lez tout.
- **LibertÃ© numÃ©rique** : HÃ©bergez vos propres outils, sans dÃ©pendance Ã  des services tiers.
- **Partage et communautÃ©** : Une plateforme qui Ã©volue grÃ¢ce Ã  vous.

---
## ğŸ³ Utilisation avec Docker


### Ã‰tapes dâ€™installation :

1. Clonez le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/gabriel25115CG/HeartSenseAI.git

2. Lancer le frontend et l'API avec Docker Compose
   ```bash 
   docker-compose up --build

ğŸ¤– Partie IA (Ollama)

Si vous souhaitez exÃ©cuter la partie IA avec Ollama, suivez ces Ã©tapes :

1. TÃ©lÃ©chargez et installez Ollama sur votre machine.
2. Une fois Ollama installÃ©, lancez le modÃ¨le Llama 3.2 avec la commande suivante :
   
   ```bash 
   ollama run llama3.2


